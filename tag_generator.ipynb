{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Generator\n",
    "A LangGraph LLM agent for generating taxonomy tag recommendations from a set of articles in a Sanity.io Content Lake instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests\n",
    "# %pip install python-dotenv\n",
    "# %pip install pandas\n",
    "# %pip install langchain\n",
    "# %pip install langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Gather article URLS\n",
    "GROQ query to gather list of articles to assess; return their URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "project_id = os.getenv(\"SANITY_PROJECT_ID\")\n",
    "dataset = \"production\"\n",
    "base_url = \"https://www.andyfitzgeraldconsulting.com/insights/\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"https://{project_id}.api.sanity.io/v2022-03-07/data/query/{dataset}?\",\n",
    "    json={\n",
    "        \"query\": f\"\"\"\n",
    "            *[_type in $types]{{\n",
    "            title,\n",
    "            \"url\": \"{base_url}\" + slug.current,\n",
    "            \"type\": insightType->prefLabel\n",
    "            }}\n",
    "        \"\"\",\n",
    "        \"params\": {\"types\": [\"article\", \"caseStudy\"]},\n",
    "    },\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    doc_list = response.json()\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch & tidy article content\n",
    "- Extract the content in the `<article>` tag. Remove header image and existing topic tags.\n",
    "- Write content and metadata to a DataFrame \n",
    "- Write content word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        response.encoding = 'utf-8'  # Set the encoding explicitly to UTF-8 to avoid HTML entity issues\n",
    "        html_content = response.text\n",
    "    else:    \n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        # Extract the <article> element\n",
    "        article = soup.find(\"article\")\n",
    "        if article:\n",
    "            # Remove existing classification and non-relevant tags\n",
    "            for tag in article.find_all([\"h2\", \"h3\", \"figure\", \"time\"]):\n",
    "                # Remove H2 and H3 topic tags and associated lists\n",
    "                if \"Topics\" in tag.get_text(strip=True):\n",
    "                    next_sibling = tag.find_next_sibling()\n",
    "                    if next_sibling and next_sibling.name == \"ul\":\n",
    "                        next_sibling.decompose()\n",
    "                    tag.decompose()\n",
    "                # Remove banner images\n",
    "                elif tag.name == \"figure\" and \"banner\" in tag.get(\"class\", []):\n",
    "                    tag.decompose()\n",
    "                # Remove publication timestamp\n",
    "                elif tag.name == \"time\":\n",
    "                    tag.decompose()\n",
    "                \n",
    "            # Convert to markdown for easier LLM processing\n",
    "            converter = html2text.HTML2Text()\n",
    "            converter.ignore_links = True\n",
    "            markdown_content = converter.handle(str(article))\n",
    "            return markdown_content\n",
    "    else:\n",
    "        print(\"No content to parse\")\n",
    "\n",
    "# Fetch the content for each document and write to a dataframe, along with the metadata\n",
    "df_articles = pd.DataFrame(doc_list[\"result\"])   \n",
    "df_articles[\"content\"] = df_articles[\"url\"].apply(fetch_page_content)\n",
    "df_articles[\"word_count\"] = df_articles[\"content\"].apply(lambda x: len(x.split()))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a list of topic tags\n",
    "- provide 10 - 20 tags, depending on the length of the resource, taking into account site purpose and audience profiles\n",
    "- for each, return:\n",
    "    - a tag label\n",
    "    - a tag definition (1-2 sentences)\n",
    "    - a sentence explanation of why it was chosen (1-3 sentences)\n",
    "    - a relevance score between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Enable tracing with LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"tag-gen\"\n",
    "\n",
    "# Set the USER_AGENT environment variable\n",
    "os.environ[\"USER_AGENT\"] = \"langchain-agent\"\n",
    "\n",
    "\n",
    "# Define the output schema for the tag generation model\n",
    "class TagOutput(BaseModel):\n",
    "    \"\"\"Return a list of tag recommendations.\"\"\"\n",
    "\n",
    "    class TagRecommendation(BaseModel):\n",
    "        \"\"\"Return data about each recommended tag.\"\"\"\n",
    "\n",
    "        tag: str = Field(description=\"The recommended tag for the document.\")\n",
    "        definition: str = Field(\n",
    "            description=\"A definition of the tag in the context of the website purpose and audience.\"\n",
    "        )\n",
    "        explanation: str = Field(\n",
    "            description=\"An explanation of why the tag is relevant to the document.\"\n",
    "        )\n",
    "        relevance: float = Field(\n",
    "            description=\"The relevance score of the tag to the document.\"\n",
    "        )\n",
    "\n",
    "    tags: list[TagRecommendation] = Field(\n",
    "        description=\"A list of recommended tags for the document, each containing the tag, explanation, and relevance score.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# OPEN_API_KEY environment variable is set in .env\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt components\n",
    "purpose = \"The site is for a single person information architecture and content strategy consultancy that offers a multi-disciplinary, collaborative, and hands-on approach to information architecture and content strategy, design, and engineering.\"\n",
    "\n",
    "audience = \"\"\"\n",
    "    1. The Mission-Driven Leader:\n",
    "        - Profile: Heads a non-profit or social enterprise focused on making a positive social impact. Values clear, effective communication and understands the need for a strong digital presence.\n",
    "        - Needs: Looking for expert guidance in structuring digital content to maximize impact, engage stakeholders, and communicate their mission.\n",
    "        - Behavior: Seeks out proven professionals with a track record in supporting similar organizations.\n",
    "\n",
    "    1. The Tech-Savvy Innovator:    \n",
    "        - Profile: Works in a technology-driven environment, possibly in a startup or an innovative corporate department. Interested in the latest trends like LLMs and KGs.\n",
    "        - Needs: Wants to integrate advanced technology into content creation and management to stay ahead in the market.\n",
    "        - Behavior: Attracted to cutting-edge solutions and thought leadership in the field of information architecture and content design.\n",
    "        \n",
    "    3. The Established Professional:\n",
    "        - Profile: A seasoned professional in a larger, established organization, possibly overseeing a content or digital marketing team.\n",
    "        - Needs: Looking for high-value strategic solutions to refine and elevate their organization's content strategy and structure.\n",
    "        - Behavior: Values expertise, reliability, and a demonstrable track record of successful projects.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "                You are a topic tagging assistant designed to suggest tags for the provided document. Suggested tags will take into consideration the main topics and themes of the document, as well as the purpose of the site, and the needs of the target audience.\n",
    "\n",
    "                The purpose of the site is as follows: \n",
    "\n",
    "                {purpose}\n",
    "\n",
    "                The target audience is as follows: \n",
    "\n",
    "                {audience}\n",
    "\n",
    "                Here is the document to be tagged: \n",
    "\n",
    "                {content}\n",
    "\n",
    "                Please provide a list of {tag_count} topical tags that describe the document, in the context of the site's purpose and target audience. When creating tags, do not use parenthetical qualifiers or acronyms. If the plural version of the term and the singular version are equally valid, use the plural form. All terms should be in lower case, unless they are proper nouns.                \n",
    "                \n",
    "                Along with each tag, please also provide a one to two sentence definition of the tag in the context of the site purpose and audience, and a one to three sentence explanation of why the tag is relevant to the document. Finally, provide a relevance score for the tag between 0.0 and 1.0, with tags more relevant to the document scoring higher.\n",
    "            \"\"\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_model = model.with_structured_output(TagOutput)\n",
    "\n",
    "chain = prompt | structured_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loop through articles\n",
    "Write results to a \"tags\" DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tags_df = pd.DataFrame()\n",
    "\n",
    "for index, row in df_articles.iloc[0:3].iterrows():\n",
    "    content, title, url, type, word_count = row[[\"content\", \"title\", \"url\", \"type\", \"word_count\"]]\n",
    "\n",
    "    # adjust the tag count requested based on the word count of the document\n",
    "    count = min(max(10, int(word_count / 400)), 20)\n",
    "    \n",
    "    response = chain.invoke(\n",
    "        {\"content\": content, \"audience\": audience, \"purpose\": purpose, \"tag_count\": count}\n",
    "    )\n",
    "\n",
    "    response_dict = response.model_dump()\n",
    "\n",
    "    # write each row of tags to the `tags_df` dataframe, and include for each row the document title, URL, and type\n",
    "    tags_df = pd.concat(\n",
    "        [tags_df, pd.DataFrame(response_dict[\"tags\"]).assign(title=title, url=url, type=type)],\n",
    "        ignore_index=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process tag list\n",
    "- remove tags that only appear once or have a relevance score below 0.6\n",
    "- remove tags that appear for every article (TO DO — once there are more articles)\n",
    "- write remaining unique tags to a new DataFrame\n",
    "    - concatenate the multiple definitions\n",
    "    - average the relevance scores\n",
    "    - count the number of content types represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# write tags_df to a CSV file for later reference in development (to limit remote calls to the LLM)\n",
    "# tags_df.to_csv(\"unprocessed_tags.csv\", index=False)\n",
    "\n",
    "# read the CSV file back in\n",
    "# tags_df = pd.read_csv(\"unprocessed_tags.csv\")\n",
    "\n",
    "# create a new DataFrame and remove any rows where the tag only appears once and the relevance score is less than 0.6\n",
    "filtered_tags_df = tags_df.groupby(\"tag\").filter(lambda x: len(x) > 1 or x[\"relevance\"].max() >= 0.6)\n",
    "\n",
    "# write remaining unique tags to a new DataFrame, synthesize the multiple explanations, average the relevance scores, and count the number of Types represented by each tag\n",
    "processed_tags_df = (\n",
    "    filtered_tags_df.groupby(\"tag\")\n",
    "    .agg(\n",
    "        tag_count=(\"title\", \"count\"),\n",
    "        relevance=(\"relevance\", \"mean\"),\n",
    "        std_dev=(\"relevance\", \"std\"),\n",
    "        type_count=(\"type\", \"nunique\"),\n",
    "        definition=(\"definition\", lambda x: \" \".join(x)),\n",
    "\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Synthesize definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize grouped explanations using Mistral on Ollama\n",
    "# %pip install ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ollama = ChatOllama(model=\"mistral\", temperature=0)\n",
    "\n",
    "# Define a function to synthesize explanations\n",
    "def synthesize_definitions(tag, definitions):\n",
    "    prompt = f\"\"\"\n",
    "    You are a content strategist and copywriter. The following tags and definitions are topic tag definitions for a website that provides information architecture and content strategy services. The definitions are not a single definition, but rather several definitions for the same tag that have been joined together. \n",
    "    \n",
    "    You are tasked with synthesizing these definitions into a single, concise definition of between three and five sentences that is easy to understand and captures the essence of the topic tag:\n",
    "\n",
    "    Here's the tag: {tag}\n",
    "\n",
    "    Here are the definitions: {definitions}\n",
    "\n",
    "    Please provide your synthesized definition below. Your definition should be between three and five sentences long. \n",
    "    \"\"\"\n",
    "    response = ollama.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Apply the function to the definitions column\n",
    "processed_tags_df[\"synthesized_definition\"] = processed_tags_df.apply(\n",
    "    lambda row: synthesize_definitions(row[\"tag\"],row[\"definition\"]) if row[\"tag_count\"] > 1 else row[\"definition\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# write the processed tags to a new CSV file\n",
    "# processed_tags_df.to_csv(\"processed_tags.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "tokenized_tags = [nlp(tag) for tag in processed_tags_df[\"tag\"].tolist()]\n",
    "\n",
    "tokenized_definitions = [nlp(definition) for definition in processed_tags_df[\"synthesized_definition\"].tolist()]\n",
    "\n",
    "# create a list of dicts with the tag and definition tokens\n",
    "tag_tokens = [\n",
    "    {\"tag\": tag, \"definition\": definition}\n",
    "    for tag, definition in zip(tokenized_tags, tokenized_definitions)\n",
    "]\n",
    "\n",
    "print(\"Similarity by Tag:\")\n",
    "for tag in tag_tokens:\n",
    "    #identify tag explanations that are similar to each other and group them together\n",
    "    for other_tag in tag_tokens:\n",
    "        if tag != other_tag and tag[\"tag\"].similarity(other_tag[\"tag\"]) > 0.98:\n",
    "            print(f\"{tag['tag'].text} is similar to {other_tag['tag'].text}\")\n",
    "\n",
    "print(\"\\nSimilarity by Definition:\")\n",
    "for tag in tag_tokens:\n",
    "    #identify tag explanations that are similar to each other and group them together\n",
    "    for other_tag in tag_tokens:\n",
    "        if tag != other_tag and tag[\"definition\"].similarity(other_tag[\"definition\"]) > 0.98:\n",
    "            print(f\"{tag['tag'].text} is similar to {other_tag['tag'].text}\")\n",
    "\n",
    "\n",
    "# for tag in tokenized_explanations:\n",
    "#     # print(tag.text, tag.has_vector, tag.vector_norm)\n",
    "#     # identify tags that are similar to each other\n",
    "#     for other_tag in tokenized_explanations:\n",
    "#         if tag != other_tag and tag.similarity(other_tag) > 0.95:\n",
    "#             print(f\"{tag.text} is similar to {other_tag.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot with relevance on the x-axis and tag_count on the y-axis and tag name available on hover\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    processed_tags_df[processed_tags_df[\"tag_count\"] > 1],\n",
    "    hover_data=[\"tag\"],\n",
    "    x=\"relevance\",\n",
    "    y=\"tag_count\",\n",
    "    title=\"Tag Relevance vs. Tag Count\",\n",
    "    width=800,\n",
    "    height=800,\n",
    "    size=\"type_count\",\n",
    "    color=\"std_dev\",\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    labels={\n",
    "        \"relevance\": \"Relevance Score\",\n",
    "        \"tag_count\": \"Number of Resources\",\n",
    "        \"type_count\": \"Number of Types\",\n",
    "        \"std_dev\": \"StdDev\",\n",
    "    },\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
