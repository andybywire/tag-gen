{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Generator\n",
    "\n",
    "An LLM workflow for generating taxonomy tag recommendations from a set of articles in a Sanity.io Content Lake instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests\n",
    "# %pip install python-dotenv\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install html2text\n",
    "# %pip install pandas\n",
    "# %pip install langchain_openai\n",
    "# %pip install langchain_core\n",
    "# %pip install numpy\n",
    "# %pip install langchain_ollama\n",
    "# %pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch & Tidy Articles\n",
    "\n",
    "- GROQ query to gather list of articles to assess; return their URLs\n",
    "- Extract the content in the `<article>` tag. Remove header image and existing topic tags.\n",
    "- Write content and metadata to a DataFrame\n",
    "- Write content word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "project_id = os.getenv(\"SANITY_PROJECT_ID\")\n",
    "dataset = \"production\"\n",
    "base_url = \"https://www.andyfitzgeraldconsulting.com/insights/\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"https://{project_id}.api.sanity.io/v2022-03-07/data/query/{dataset}?\",\n",
    "    json={\n",
    "        \"query\": f\"\"\"\n",
    "            *[_type in $types]{{\n",
    "            title,\n",
    "            \"url\": \"{base_url}\" + slug.current,\n",
    "            \"type\": insightType->prefLabel\n",
    "            }}\n",
    "        \"\"\",\n",
    "        \"params\": {\"types\": [\"article\", \"caseStudy\"]},\n",
    "    },\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    doc_list = response.json()\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        response.encoding = (\n",
    "            \"utf-8\"  # Set the encoding explicitly to UTF-8 to avoid HTML entity issues\n",
    "        )\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        # Extract the <article> element\n",
    "        article = soup.find(\"article\")\n",
    "        if article:\n",
    "            # Remove existing classification and non-relevant tags\n",
    "            for tag in article.find_all([\"h2\", \"h3\", \"figure\", \"time\"]):\n",
    "                # Remove H2 and H3 topic tags and associated lists\n",
    "                if \"Topics\" in tag.get_text(strip=True):\n",
    "                    next_sibling = tag.find_next_sibling()\n",
    "                    if next_sibling and next_sibling.name == \"ul\":\n",
    "                        next_sibling.decompose()\n",
    "                    tag.decompose()\n",
    "                # Remove banner images\n",
    "                elif tag.name == \"figure\" and \"banner\" in tag.get(\"class\", []):\n",
    "                    tag.decompose()\n",
    "                # Remove publication timestamp\n",
    "                elif tag.name == \"time\":\n",
    "                    tag.decompose()\n",
    "\n",
    "            # Convert to markdown for easier LLM processing\n",
    "            converter = html2text.HTML2Text()\n",
    "            converter.ignore_links = True\n",
    "            markdown_content = converter.handle(str(article))\n",
    "            return markdown_content\n",
    "    else:\n",
    "        print(\"No content to parse\")\n",
    "\n",
    "\n",
    "# Fetch the content for each document and write to a dataframe, along with the metadata\n",
    "df_articles = pd.DataFrame(doc_list[\"result\"])\n",
    "df_articles[\"content\"] = df_articles[\"url\"].apply(fetch_page_content)\n",
    "df_articles[\"word_count\"] = df_articles[\"content\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Tag Prompt & Chain\n",
    "\n",
    "- provide 10 - 20 tags, depending on the length of the resource, taking into account site purpose and audience profiles\n",
    "- for each, return:\n",
    "  - a tag label\n",
    "  - a tag definition (1-2 sentences)\n",
    "  - a sentence explanation of why it was chosen (1-3 sentences)\n",
    "  - a relevance score between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Enable tracing with LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"tag-gen\"\n",
    "\n",
    "# Set the USER_AGENT environment variable\n",
    "os.environ[\"USER_AGENT\"] = \"langchain-agent\"\n",
    "\n",
    "\n",
    "# Define the output schema for the tag generation model\n",
    "class TagOutput(BaseModel):\n",
    "    \"\"\"Return a list of tag recommendations.\"\"\"\n",
    "\n",
    "    class TagRecommendation(BaseModel):\n",
    "        \"\"\"Return data about each recommended tag.\"\"\"\n",
    "\n",
    "        tag: str = Field(description=\"The recommended tag for the document.\")\n",
    "        definition: str = Field(\n",
    "            description=\"A definition of the tag in the context of the website purpose and audience.\"\n",
    "        )\n",
    "        explanation: str = Field(\n",
    "            description=\"An explanation of why the tag is relevant to the document.\"\n",
    "        )\n",
    "        relevance: float = Field(\n",
    "            description=\"The relevance score of the tag to the document.\"\n",
    "        )\n",
    "\n",
    "    tags: list[TagRecommendation] = Field(\n",
    "        description=\"A list of recommended tags for the document, each containing the tag, explanation, and relevance score.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# OPEN_API_KEY environment variable is set in .env\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt components\n",
    "purpose = \"The site is for a single person information architecture and content strategy consultancy that offers a multi-disciplinary, collaborative, and hands-on approach to information architecture and content strategy, design, and engineering.\"\n",
    "\n",
    "audience = \"\"\"\n",
    "    1. The Mission-Driven Leader:\n",
    "        - Profile: Heads a non-profit or social enterprise focused on making a positive social impact. Values clear, effective communication and understands the need for a strong digital presence.\n",
    "        - Needs: Looking for expert guidance in structuring digital content to maximize impact, engage stakeholders, and communicate their mission.\n",
    "        - Behavior: Seeks out proven professionals with a track record in supporting similar organizations.\n",
    "\n",
    "    1. The Tech-Savvy Innovator:    \n",
    "        - Profile: Works in a technology-driven environment, possibly in a startup or an innovative corporate department. Interested in the latest trends like LLMs and KGs.\n",
    "        - Needs: Wants to integrate advanced technology into content creation and management to stay ahead in the market.\n",
    "        - Behavior: Attracted to cutting-edge solutions and thought leadership in the field of information architecture and content design.\n",
    "        \n",
    "    3. The Established Professional:\n",
    "        - Profile: A seasoned professional in a larger, established organization, possibly overseeing a content or digital marketing team.\n",
    "        - Needs: Looking for high-value strategic solutions to refine and elevate their organization's content strategy and structure.\n",
    "        - Behavior: Values expertise, reliability, and a demonstrable track record of successful projects.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "                You are a topic tagging assistant designed to suggest tags for the provided document. Suggested tags will take into consideration the main topics and themes of the document, as well as the purpose of the site, and the needs of the target audience.\n",
    "\n",
    "                The purpose of the site is as follows: \n",
    "\n",
    "                {purpose}\n",
    "\n",
    "                The target audience is as follows: \n",
    "\n",
    "                {audience}\n",
    "\n",
    "                Here is the document to be tagged: \n",
    "\n",
    "                {content}\n",
    "\n",
    "                Please provide a list of {tag_count} topical tags that describe the document, in the context of the site's purpose and target audience. When creating tags, do not use parenthetical qualifiers or acronyms. If the plural version of the term and the singular version are equally valid, use the plural form. All terms should be in lower case, unless they are proper nouns.                \n",
    "                \n",
    "                Along with each tag, please also provide a one to two sentence definition of the tag in the context of the site purpose and audience, and a one to three sentence explanation of why the tag is relevant to the document. Finally, provide a relevance score for the tag between 0.0 and 1.0, with tags more relevant to the document scoring higher.\n",
    "            \"\"\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_model = model.with_structured_output(TagOutput)\n",
    "\n",
    "chain = prompt | structured_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Tags\n",
    "\n",
    "- Write results to a \"tags\" DataFrame\n",
    "- Export unprocessed tags for reference in taxonomy development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tags_df = pd.DataFrame()\n",
    "\n",
    "# use `df_articles.iloc[0:3].iterrows()` to iterate over a smaller number of rows for prompt testing\n",
    "for index, row in df_articles.iterrows():\n",
    "    content, title, url, type, word_count = row[\n",
    "        [\"content\", \"title\", \"url\", \"type\", \"word_count\"]\n",
    "    ]\n",
    "\n",
    "    # adjust the tag count requested based on the word count of the document\n",
    "    count = min(max(10, int(word_count / 400)), 20)\n",
    "\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"content\": content,\n",
    "            \"audience\": audience,\n",
    "            \"purpose\": purpose,\n",
    "            \"tag_count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response_dict = response.model_dump()\n",
    "\n",
    "    # write each row of tags to the `tags_df` dataframe, and include for each row the document title, URL, and type\n",
    "    tags_df = pd.concat(\n",
    "        [\n",
    "            tags_df,\n",
    "            pd.DataFrame(response_dict[\"tags\"]).assign(title=title, url=url, type=type),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "# This file is for taxonomy development, but can also be used for later reference in development to limit remote calls to the LLM\n",
    "tags_df.to_csv(\"unprocessed_tags.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Tag List\n",
    "\n",
    "- remove tags that only appear once or have a relevance score below 0.6\n",
    "- remove tags that appear for every article (TO DO â€” once there are more articles)\n",
    "- write remaining unique tags to a new DataFrame\n",
    "  - concatenate the multiple definitions\n",
    "  - average the relevance scores\n",
    "  - count the number of content types represented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV file back in if iterating workflow\n",
    "# import pandas as pd\n",
    "# tags_df = pd.read_csv(\"unprocessed_tags.csv\")\n",
    "\n",
    "# create a new DataFrame and remove any rows where the tag only appears once and the relevance score is less than 0.6\n",
    "filtered_tags_df = tags_df.groupby(\"tag\").filter(\n",
    "    lambda x: len(x) > 1 and x[\"relevance\"].max() >= 0.6\n",
    ")\n",
    "\n",
    "# write remaining unique tags to a new DataFrame, synthesize the multiple explanations, average the relevance scores, and count the number of Types represented by each tag\n",
    "processed_tags_df = (\n",
    "    filtered_tags_df.groupby(\"tag\")\n",
    "    .agg(\n",
    "        tag_count=(\"title\", \"count\"),\n",
    "        relevance=(\"relevance\", \"mean\"),\n",
    "        std_dev=(\"relevance\", \"std\"),\n",
    "        type_count=(\"type\", \"nunique\"),\n",
    "        definition=(\"definition\", lambda x: \" \".join(x)),\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synthesize Definitions\n",
    "\n",
    "- This is a less complex synthesis task with far lower token counts, so can be reasonably handled by a locally hosted LLM\n",
    "- Synthesized definitions are added in a new column in the DataFrame. Unchanged definitions are passed through to this column for consistency.\n",
    "- Export synthesized tags for reference in taxonomy development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize grouped explanations using Mistral on Ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "import pandas as pd\n",
    "\n",
    "ollama = ChatOllama(model=\"mistral\", temperature=0)\n",
    "\n",
    "\n",
    "# Define a function to synthesize explanations\n",
    "def synthesize_definitions(tag, definitions):\n",
    "    prompt = f\"\"\"\n",
    "    You are a content strategist and copywriter. The following tags and definitions are topic tag definitions for a website that provides information architecture and content strategy services. The definitions are not a single definition, but rather several definitions for the same tag that have been joined together. \n",
    "    \n",
    "    You are tasked with synthesizing these definitions into a single, concise definition of between three and five sentences that is easy to understand and captures the essence of the topic tag:\n",
    "\n",
    "    Here's the tag: {tag}\n",
    "\n",
    "    Here are the definitions: {definitions}\n",
    "\n",
    "    Please provide your synthesized definition below. Your definition should be between three and five sentences long. \n",
    "    \"\"\"\n",
    "    response = ollama.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Apply the function to the definitions column\n",
    "processed_tags_df[\"synthesized_definition\"] = processed_tags_df.apply(\n",
    "    lambda row: (\n",
    "        synthesize_definitions(row[\"tag\"], row[\"definition\"])\n",
    "        if row[\"tag_count\"] > 1\n",
    "        else row[\"definition\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# write the processed tags to a new CSV file\n",
    "processed_tags_df.to_csv(\"processed_tags.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize & Export Results\n",
    "\n",
    "- Create an interactive scatter plot to visualize tag count, relevance, and standard deviation of relevance scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot with relevance on the x-axis and tag_count on the y-axis and tag name available on hover\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    processed_tags_df[processed_tags_df[\"tag_count\"] > 1],\n",
    "    hover_data=[\"tag\"],\n",
    "    x=\"relevance\",\n",
    "    y=\"tag_count\",\n",
    "    title=\"Tag Relevance vs. Tag Count\",\n",
    "    width=800,\n",
    "    height=800,\n",
    "    size=\"type_count\",\n",
    "    color=\"std_dev\",\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    labels={\n",
    "        \"relevance\": \"Relevance Score\",\n",
    "        \"tag_count\": \"Number of Resources\",\n",
    "        \"type_count\": \"Number of Types\",\n",
    "        \"std_dev\": \"StdDev\",\n",
    "    },\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"tag-results.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trimmed Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Consolidation\n",
    "Not integrated into the tag generation flow. From a broad language perspective, all tags and descriptions in this narrow set are so similar that the cutoff for any meaningful distinctions based on semantic similarity is razor thin and risks obfuscating nuances that are better off interrogated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use natural language processing to identify similar tags and definitions\n",
    "import spacy\n",
    "\n",
    "# Download the spaCy model with `python -m spacy download en_core_web_lg`\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "tokenized_tags = [nlp(tag) for tag in processed_tags_df[\"tag\"].tolist()]\n",
    "\n",
    "tokenized_definitions = [nlp(definition) for definition in processed_tags_df[\"synthesized_definition\"].tolist()]\n",
    "\n",
    "# create a list of dicts with the tag and definition tokens\n",
    "tag_tokens = [\n",
    "    {\"tag\": tag, \"definition\": definition}\n",
    "    for tag, definition in zip(tokenized_tags, tokenized_definitions)\n",
    "]\n",
    "\n",
    "print(\"Similarity by Tag:\")\n",
    "for tag in tag_tokens:\n",
    "    #identify tag explanations that are similar to each other and group them together\n",
    "    for other_tag in tag_tokens:\n",
    "        if tag != other_tag and tag[\"tag\"].similarity(other_tag[\"tag\"]) > 0.9:\n",
    "            print(f\"{tag['tag'].text} is similar to {other_tag['tag'].text}\")\n",
    "\n",
    "print(\"\\nSimilarity by Definition:\")\n",
    "for tag in tag_tokens:\n",
    "    #identify tag explanations that are similar to each other and group them together\n",
    "    for other_tag in tag_tokens:\n",
    "        if tag != other_tag and tag[\"definition\"].similarity(other_tag[\"definition\"]) > 0.9:\n",
    "            print(f\"{tag['tag'].text} is similar to {other_tag['tag'].text}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
